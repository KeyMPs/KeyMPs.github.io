# KeyMPs:<br>Keyword Labeled Primitive Selection and Keypoint Pairs Generation Guided Movement Primitives Using Vision-Language Models

![Method Overview](assets/img/method-overview.png)

Dynamic Movement Primitives (DMPs) provide a flexible framework for robotic motion generation, enabling the compact encoding of complex movements. However, they lack the inherent capability to associate with multimodal data such as vision or linguistic instructions, and additionally do not possess inherent spatial awareness without external integration.
Vision-Language Models (VLMs), on the other hand, could effectively bridge multimodal data, such as linguistic instructions and vision, facilitating connections to lower-level control frameworks to improve task planning and execution. However, they struggle with accurately generating low-level information needed by robots due to their absence in training data.
To address these challenges, we propose **Keyword Labeled Primitive Selection and Keypoint Pairs Generation Guided Movement Primitives (KeyMPs)**, a framework that leverages VLMs to infer DMP parameters by handling *learned parameters* and *spatial scaling parameters* separately. This separation allows VLMs to utilize their high-level reasoning capabilities to manage *learned parameters* through selection based on assigned keywords, while also leveraging VLMs' spatial awareness to enhance generalization and reduce reliance on extensive human demonstrations by generating *spatial scaling parameters*.
We validate our approach through various experiments conducted on the object cutting task in both simulated and real-world environments, demonstrating superior performance over DMP-based comparison methods that also utilize VLM support in generating accurate and complex motions.

---

## Method

Our proposed framework, **KeyMPs**, integrates language and vision inputs to generate executable motions by leveraging VLMs and DMPs. It operates through three stages:

1. **Input acquisition:**
   - The framework begins by collecting two primary types of inputs:
     - *Language input*: User-provided instructions.
     - *Vision input*: Environment or object representations from a camera.
   - An object detector identifies the object's global coordinates and produces a cropped object image, while the object's height measurement is supplied to the framework.
   - A pixel-based object detection approach determines the global coordinates (detailed in Appendix A). Alternative object detection methods are also supported.

2. **Contextual processing:**
   - VLMs handle both *learned* and *spatial scaling parameters* separately to generate DMP parameters.
   - During **keyword labeled primitive selection**, VLMs employ high-level reasoning to select a reference primitive from a predefined primitive dictionary.
   - For **keypoint pairs generation**, VLMs generate 2D keypoint pairs that, combined with the object's height, define the *spatial scaling parameters* (\( y_0 \) and \( y_{\text{goal}} \)). These are based on combined language and vision inputs.

3. **DMPs motion generation:**
   - The reference primitive and the *spatial scaling parameters* are used to create the definitive DMP motion.
   - The motion is generated by iteratively applying each spatial scaling parameter to scale the reference primitive.

---

### Additional Information
The rest of the details can be found in our [research paper](https://arxiv.org/abs/2504.10011).

---

## Videos
Demonstration videos can be seen in our [project website](https://KeyMPs.github.io) 
