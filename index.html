<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="KeyMPs: One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks. Learn about our framework combining VLMs and DMPs for complex robotic motion generation.">
    <meta name="keywords" content="KeyMPs, robotics, motion generation, DMPs, VLMs, vision-language models, occlusion-rich, one-shot learning">
    <meta name="author" content="EDGAR ANAROSSI, YUHWAN KWON, HIROTAKA TAHARA, SHOHEI TANAKA, KEISUKE SHIRAI, MASASHI HAMAYA, CHRISTIAN BELTRAN, ATSUSHI HASHIMOTO, TAKAMITSU MATSUBARA">
    <title>KeyMPs: One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks</title>
    <link rel="icon" href="assets/favicon_64x64.png" type="image/x-icon">
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400&family=Crimson+Pro:wght@400&family=Cormorant+Garamond:wght@400&family=Libre+Baskerville:wght@400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/basiclightbox@5.0.4/dist/basicLightbox.min.css">
    <link rel="stylesheet" href="assets/style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // Initialize ClipboardJS for all copy buttons
            const clipboard = new ClipboardJS('.copy-button');

            clipboard.on('success', (e) => {
                console.log('Text copied successfully:', e.text);
                e.clearSelection();
            });
            clipboard.on('error', (e) => {
                console.error('Copy failed:', e.action);
            });

            // Standardize height for .text-box elements
            const textBoxes = document.querySelectorAll('.text-box');
            let maxHeightTextBox = 0;
            textBoxes.forEach((textBox) => {
                textBox.style.height = 'auto';
                const contentHeight = textBox.scrollHeight;
                maxHeightTextBox = Math.max(maxHeightTextBox, contentHeight);
            });
            textBoxes.forEach((textBox) => {
                textBox.style.height = `${maxHeightTextBox}px`;
            });

            // Standardize height for .text-box-merge elements
            const textBoxesMerge = document.querySelectorAll('.text-box-merge');
            let maxHeightMerge = 0;
            textBoxesMerge.forEach((textBox) => {
                textBox.style.height = 'auto';
                const contentHeight = textBox.scrollHeight;
                maxHeightMerge = Math.max(maxHeightMerge, contentHeight);
            });
            textBoxesMerge.forEach((textBox) => {
                textBox.style.height = `${maxHeightMerge}px`;
            });

            // Smooth scrolling with offset for navigation links
            document.querySelectorAll('nav a').forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    const targetId = this.getAttribute('href').substring(1);

                    if (targetId === 'home') {
                        window.scrollTo({
                            top: 0,
                            behavior: 'smooth'
                        });
                    } else {
                        const targetElement = document.getElementById(targetId);
                        const offset = 60;
                        const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - offset;

                        window.scrollTo({
                            top: targetPosition,
                            behavior: 'smooth'
                        });
                    }
                });
            });

            // Function to load content from a file
            async function loadContent(elementId, filePath) {
                try {
                    const response = await fetch(filePath);
                    if (!response.ok) {
                        throw new Error(`HTTP error! status: ${response.status}`);
                    }
                    const content = await response.text();
                    document.getElementById(elementId).innerHTML = content;
                    // Re-initialize clipboard and lightbox for the new content
                    initializeClipboard();
                    initializeLightbox();
                    standardizeTextBoxHeights(); // Re-standardize heights after loading content
                } catch (error) {
                    console.error(`Could not load content from ${filePath}:`, error);
                    document.getElementById(elementId).innerHTML = `<p>Error loading content.</p>`;
                }
            }

            // Function to initialize clipboard for newly loaded content
            function initializeClipboard() {
                 const clipboard = new ClipboardJS('.copy-button');
                 clipboard.on('success', (e) => {
                    console.log('Text copied successfully:', e.text);
                    e.clearSelection();
                });
                clipboard.on('error', (e) => {
                    console.error('Copy failed:', e.action);
                });
            }

            // Function to initialize lightbox for newly loaded content
            function initializeLightbox() {
                 document.querySelectorAll('img').forEach(img => {
                    // Avoid adding multiple listeners to the same image
                    if (!img.dataset.lightboxInitialized) {
                        img.addEventListener('click', () => {
                            const figure = img.closest('figure');
                            const caption = figure ? figure.querySelector('figcaption')?.textContent : '';
                            const captionHtml = caption ? `<figcaption>${caption}</figcaption>` : '';
                            const instance = basicLightbox.create(`
                                <div class="modal">
                                    <img class="img-close" src="${img.src}" alt="${img.alt}" style="max-width: 90vw; max-height: 90vh;">
                                    ${captionHtml}
                                    <a class="lightbox-close" onclick="this.parentElement.parentElement.close()">×</a>
                                </div>
                            `, {
                                onShow: (instance) => {
                                    instance.element().querySelector('.lightbox-close').onclick = instance.close;
                                }
                            });
                            instance.show();
                        });
                        img.dataset.lightboxInitialized = 'true'; // Mark as initialized
                    }
                });
            }

            // Function to standardize text box heights for newly loaded content
            function standardizeTextBoxHeights() {
                const textBoxes = document.querySelectorAll('.text-box');
                let maxHeightTextBox = 0;
                textBoxes.forEach((textBox) => {
                    textBox.style.height = 'auto';
                    const contentHeight = textBox.scrollHeight;
                    maxHeightTextBox = Math.max(maxHeightTextBox, contentHeight);
                });
                textBoxes.forEach((textBox) => {
                    textBox.style.height = `${maxHeightTextBox}px`;
                });

                const textBoxesMerge = document.querySelectorAll('.text-box-merge');
                let maxHeightMerge = 0;
                textBoxesMerge.forEach((textBox) => {
                    textBox.style.height = 'auto';
                    const contentHeight = textBox.scrollHeight;
                    maxHeightMerge = Math.max(maxHeightMerge, contentHeight);
                });
                textBoxesMerge.forEach((textBox) => {
                    textBox.style.height = `${maxHeightMerge}px`;
                });
            }


            // Tab switching functionality
            const tabButtons = document.querySelectorAll('.tab-button');
            const tabContents = document.querySelectorAll('.tab-content');

            // Map tab IDs to file paths
            const tabFileMap = {
                'cutting': 'assets/cutting_experiment.html',
                'icing': 'assets/icing_experiment.html'
            };

            tabButtons.forEach(button => {
                button.addEventListener('click', () => {
                    const tabId = button.getAttribute('data-tab');

                    // Deactivate all buttons and contents
                    tabButtons.forEach(btn => btn.classList.remove('active'));
                    tabContents.forEach(content => content.classList.remove('active'));

                    // Activate the clicked button
                    button.classList.add('active');

                    // Activate the corresponding content div
                    const targetContent = document.getElementById(tabId);
                    targetContent.classList.add('active');

                    // Load content if it hasn't been loaded yet
                    if (!targetContent.dataset.loaded) {
                        const filePath = tabFileMap[tabId];
                        if (filePath) {
                            loadContent(tabId, filePath).then(() => {
                                targetContent.dataset.loaded = 'true'; // Mark as loaded
                            });
                        }
                    }
                });
            });

            // New submethod tab-switching JavaScript
            const submethodTabButtons = document.querySelectorAll('.submethod-tab-button');
            const submethodTabContents = document.querySelectorAll('.submethod-tab-content');
            submethodTabButtons.forEach(button => {
                button.addEventListener('click', () => {
                    const tabId = button.getAttribute('data-tab');
                    submethodTabButtons.forEach(btn => btn.classList.remove('active'));
                    submethodTabContents.forEach(content => content.classList.remove('active'));
                    button.classList.add('active');
                    document.getElementById(tabId).classList.add('active');
                });
            });

            // Initial load: Load the content for the default active tab ('cutting')
            const initialTabButton = document.querySelector('.tab-button.active');
            if (initialTabButton) {
                const initialTabId = initialTabButton.getAttribute('data-tab');
                const initialFilePath = tabFileMap[initialTabId];
                if (initialFilePath) {
                    loadContent(initialTabId, initialFilePath).then(() => {
                         document.getElementById(initialTabId).dataset.loaded = 'true';
                    });
                }
            }

            // Initialize lightbox for images already present on page load (like the method overview)
            initializeLightbox();
        });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/basiclightbox@5.0.4/dist/basicLightbox.min.js"></script>
</head>
<body>
        <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#method">Method</a></li>
                <li><a href="#experiments">Experiments</a></li>
            </ul>
        </nav>

        <div style="padding:0 20px">
            <h1><span class="title-highlight"><strong>KeyMPs</strong></span><br>One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks</h1>

            <div class="author-grid">
                <div class="author">EDGAR ANAROSSI<span><a href="https://isw3.naist.jp/" target="_blank">NAIST</a> - <a href="https://sites.google.com/view/naist-robot-learning-jp/%E3%83%9B%E3%83%BC%E3%83%A0" target="_blank">Robot Learning Lab</a></span></div>
                <div class="author">YUHWAN KWON<span><a href="https://isw3.naist.jp/" target="_blank">NAIST</a> - <a href="https://sites.google.com/view/naist-robot-learning-jp/%E3%83%9B%E3%83%BC%E3%83%A0" target="_blank">Robot Learning Lab</a><br><a href="https://www.kansai-u.ac.jp/Fc_sci/english/department/ee/major.html" target="_blank">Kansai University</a></span></div>
                <div class="author">HIROTAKA TAHARA<span><a href="https://isw3.naist.jp/" target="_blank">NAIST</a> - <a href="https://sites.google.com/view/naist-robot-learning-jp/%E3%83%9B%E3%83%BC%E3%83%A0" target="_blank">Robot Learning Lab</a><br><a href="https://www.kobe-kosen.ac.jp/groups/denshi/?page=1&cat=PC%E9%96%A2%E9%80%A3" target="_blank">Kobe Kosen</a></span></div>
                <div class="author">SHOHEI TANAKA<span><a href="https://www.omron.com/sinicx/" target="_blank">OMRON SINIC X Corporation</a></span></div>
                <div class="author">KEISUKE SHIRAI<span><a href="https://www.omron.com/sinicx/" target="_blank">OMRON SINIC X Corporation</a></span></div>
                <div class="author">MASASHI HAMAYA<span><a href="https://www.omron.com/sinicx/" target="_blank">OMRON SINIC X Corporation</a></span></div>
                <div class="author">CHRISTIAN BELTRAN<span><a href="https://www.omron.com/sinicx/" target="_blank">OMRON SINIC X Corporation</a></span></div>
                <div class="author">ATSUSHI HASHIMOTO<span><a href="https://www.omron.com/sinicx/" target="_blank">OMRON SINIC X Corporation</a></span></div>
                <div class="author">TAKAMITSU MATSUBARA<span><a href="https://isw3.naist.jp/" target="_blank">NAIST</a> - <a href="https://sites.google.com/view/naist-robot-learning-jp/%E3%83%9B%E3%83%BC%E3%83%A0" target="_blank">Robot Learning Lab</a></span></div>
                <div class="author">Jan 15, 2025</div>
                <div class="links">
                    <span class="ieee-link"><a href="" target="_blank">IEEE</a></span><br>
                    <span class="arxiv-link"><a href="https://arxiv.org/abs/2504.10011" target="_blank">Arxiv</a></span>
                </div>
            </div>
        </div>

        <div class="content">
            <figure>
                <img src="assets/img/method-overview.png" alt="Method Overview" width="80%">
                <figcaption>Overview of the KeyMPs framework for vision-language guided motion generation.</figcaption>
            </figure>
            <br>
            <h2 id="abstract"><b><u>Abstract</u></b></h2>
            <p>
                Dynamic Movement Primitives (DMPs) provide a flexible framework for encoding smooth robotic movements; however, they face challenges in integrating multimodal inputs commonly used in robotics like vision and language into their framework.
                To fully maximize DMPs' potential, it's crucial to enable DMPs to respond to such multimodal inputs and to also broaden their capability to handle object-focused tasks requiring complex motion planning in one shot, as observation occlusion could easily happen mid-execution in such tasks (e.g. knife occlusion in ingredient cutting, piping bag occlusion in cake icing, hand occlusion in dough kneading, etc.).
                A promising approach is to leverage Vision-Language Models (VLMs), which process multimodal data and grasp high-level concepts.
                However, they typically lack the knowledge and capabilities to directly infer low-level motion details and instead serve as a bridge between high-level instructions and low-level control.
                To address this limitation, we propose Keyword Labeled Primitive Selection and Keypoint Pairs Generation Guided Movement Primitives (KeyMPs), a framework that combines VLMs with sequencing DMPs.
                KeyMPs use VLMs' high-level reasoning to select a reference primitive through <i>Keyword Labeled Primitive Selection</i> and VLMs' spatial awareness to generate spatial scaling parameters used to generalize the overall motion through <i>Keypoint Pairs Generation</i>, ultimately enabling one-shot vision-language guided motion generation that aligns with the intent expressed in the multimodal input.
                We validate our approach through an occlusion-rich manipulation task, specifically object cutting experiments in both simulated and real-world environments, demonstrating superior performance over other DMP-based methods that integrate VLMs support.
            </p>

            <br>
            <h2 id="method"><b><u>Method</u></b></h2>
            <p>
                Our proposed framework, <strong>KeyMPs</strong>, integrates language and vision inputs to generate executable motions by leveraging VLMs and DMPs. It operates through three stages:
            </p>
            <div class="method-box" style="padding:20px">
                <h3><b>1. Pre-Processing:</b></h3>
                <div class="submethod">
                    <p>
                        The framework begins by collecting two primary types of inputs:
                    </p>
                    <ul>
                        <li><em>Language input:</em> User-provided instructions.</li>
                        <li><em>Vision input:</em> Environment or object representations from a camera.</li>
                    </ul>
                    <p>
                        An object detector identifies the object's global coordinates and produces a cropped object image, while the object's height measurement is supplied to the framework.
                        A pixel-based object detection approach determines the global coordinates. Alternative object detection methods are also supported.
                    </p>
                    <figure>
                        <img src="assets/img/method-1-input.png" alt="Pre-Processing" width="60%" onclick="showLightbox()">
                        <figcaption>Pre-processing of vision and textual input data.</figcaption>
                    </figure>
                </div>

                <br>
                <h3><b>2. Contextual processing:</b></h3>
                <div class="submethod-tabs">
                    <div class="submethod-tab-titles">
                        <button class="submethod-tab-button active" data-tab="keyword-selection">Keyword Labeled Primitive Selection</button>
                        <button class="submethod-tab-button" data-tab="keypoint-generation">Keypoint Pairs Generation</button>
                    </div>
                    <div class="submethod-tab-content active" id="keyword-selection">
                        <div class="submethod">
                            <p class="prompt-description">In <strong>keyword labeled primitive selection</strong>, VLMs employ high-level reasoning to select a reference primitive from a predefined primitive dictionary.</p>
                            <figure>
                                <img src="assets/img/method-2-keyword.png" alt="Keyword Labeled Primitive Selection" width="62%">
                                <figcaption>Selection of DMP primitive using VLMs.</figcaption>
                            </figure>
                            <div class="copy-bar">
                                <button class="copy-button" data-clipboard-target="#textbox-1">Copy</button>
                            </div>
                            <textarea id="textbox-1" class="text-box" readonly>
You are a 𝑡𝑎𝑠𝑘 and a robot expert.

You will be provided with an image of an 𝑜𝑏𝑗𝑒𝑐𝑡 and a user input of 𝑑𝑒𝑠𝑖𝑟𝑒𝑑 𝑜𝑢𝑡𝑐𝑜𝑚𝑒.

Your job is to select the most suitable 𝒑𝒓𝒊𝒎𝒊𝒕𝒊𝒗𝒆 from a list of 𝒑𝒓𝒊𝒎𝒊𝒕𝒊𝒗𝒆 𝒌𝒆𝒚𝒘𝒐𝒓𝒅𝒔 given the type of 𝒐𝑏𝑗𝑒𝑐𝑡 shown in the image and the user’s 𝒅𝒆𝒔𝒊𝒓𝒆𝒅 𝒐𝒖𝒕𝒄𝒐𝒎𝒆.

Here are the list of 𝑝𝑟𝑖𝑚𝑖𝑡𝑖𝑣𝑒 𝑘𝑒𝑦𝑤𝑜𝑟𝑑𝑠 for this 𝑡𝑎𝑠𝑘 : […]

Provide me with the 𝑝𝑟𝑖𝑚𝑖𝑡𝑖𝑣𝑒 𝑘𝑒𝑦𝑤𝑜𝑟𝑑 you selected. Here are some examples: …
                            </textarea>
                        </div>
                    </div>
                    <div class="submethod-tab-content" id="keypoint-generation">
                        <div class="submethod">
                            <p class="prompt-description">In <strong>keypoint pairs generation</strong>, VLMs generate 2D keypoint pairs that, combined with the object's height, define the <em>spatial scaling parameters</em> (\( y_0 \) and \( y_{\text{goal}} \)). These are based on combined language and vision inputs.</p>
                                <figure>
                                    <img src="assets/img/method-2-keypoint.png" alt="Keypoint Pairs Generation" width="62%">
                                    <figcaption>Generation of keypoint pairs design using VLMs.</figcaption>
                                </figure>
                            <div class="copy-bar">
                                    <button class="copy-button" data-clipboard-target="#textbox-2">Copy</button>
                                </div>
                                <textarea id="textbox-2" class="text-box" readonly>
You are a 𝑡𝑎𝑠𝑘 and a robot expert. You will be provided with an image of an 𝑜𝑏𝑗𝑒𝑐𝑡 and a user input of 𝒅𝒆𝒔𝒊𝒓𝒆𝒅 𝒐𝒖𝒕𝒄𝒐𝒎𝒆.

Your job is to generate keypoint pairs (lines) design(s) according to the user desired outcome.

In this 𝑡𝑎𝑠𝑘, the keypoint pairs represent 𝑣𝑒𝑟𝑏 where the starting keypoint represent the start of 𝑣𝑒𝑟𝑏 and the end keypoint represent the end of 𝑣𝑒𝑟𝑏.

To make sure proper keypoint pairs design generation, follow these steps:

1. Identify the 𝑜𝑏𝑗𝑒𝑐𝑡 in the image.

2. Describe the shape of the 𝑜𝑏𝑗𝑒𝑐𝑡 shown in the image (Rectangular? Circular? Object-specific shape?)

3. Describe your design plan to generate keypoint pairs based on the shape in no.2 and the user input to achieve the 𝒅𝒆𝒔𝒊𝒓𝒆𝒅 𝒐𝒖𝒕𝒄𝒐𝒎𝒆.

4. Make a python code to generate list of lines (list of list of coordinates) based on the plan in no.3. Make sure the code output a JSON file filled with the keypoint pairs within the range of [0, 1]. Here are some examples: …
                                </textarea>
                        </div>
                    </div>
                </div>
                <br>
                <h3><b>3. DMPs motion generation:</b></h3>
                <div class="submethod" style="padding:20px">
                    <p>
                        The reference primitive and the <em>spatial scaling parameters</em> are used to create the definitive DMP motion.
                        The motion is generated by iteratively applying each spatial scaling parameter to scale the reference primitive.
                    </p>
                    <figure>
                        <img src="assets/img/method-3-sequencing.png" alt="DMP-based Motion Generation" width="60%" onclick="showLightbox()">
                        <figcaption>Sequencing of scaled DMP primitives to create the final complex motion.</figcaption>
                    </figure>
                </div>
            </div>
            <p>The rest of the details can be found in our <a href="#">research paper</a>.</p>
        </div>
        <br>
        <div class="experiments-section">
            <h2 id="experiments"><b><u>Experiments</u></b></h2>
            <div class="tabs">
                <div class="tab-titles">
                    <button class="tab-button active" data-tab="cutting">Object Cutting Task</button>
                    <button class="tab-button" data-tab="icing">Cake Icing Task</button>
                </div>
                <div class="tab-content active" id="cutting">
                    <!-- Content for Object Cutting Task will be loaded here by JavaScript -->
                </div>
                <div class="tab-content" id="icing">
                    <!-- Content for Cake Icing Task will be loaded here by JavaScript -->
                </div>
            </div>
        </div>


    </body>
</html>